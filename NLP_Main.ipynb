{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "C-GP1j3hAeZ5"
   },
   "source": [
    "Author: *Ren Yang*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-Lljp-86w7z"
   },
   "source": [
    "### Objective \n",
    "- In this project, we aim to practice using python to implement some famous NLP models\n",
    "to classify tweets that are about real natural disasters versus those that are not. Training\n",
    "and testing datasets are provided by Kaggle.com. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNm8XFET7SQ6"
   },
   "source": [
    "### Data and Methodology\n",
    "- We use the data provided by Kaggle.com Competition: \"Natural Language Processing with Disaster Tweets\" https://www.kaggle.com/c/nlp-getting-started/data. The dataset contains text-based, unequal length tweets about natrual disasters. The training set consists of tweets and lablels( two classes 0,1, indicating if that tweet is about a real diaster or not).\n",
    "\n",
    "- We choose 3 deep learning models for this natural language classfication problems:\n",
    " 1. Single dense layer model; \n",
    " 2. Transfer learning: Single dense layer model with pretained text embedding layer; \n",
    " 3. Transfer learning: 1-dimensional CNN model with pretrained text embedding layer. \n",
    "\n",
    "For pretrain encoder, we choose the Universial Sentence Encoder (USE) as our pretrain text embedder. \n",
    "\n",
    "We compare the performance between these models in terms of training time, prediction accuracy, etc. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_PZCKfcztE4"
   },
   "source": [
    "### Main Program\n",
    "In our main program, we created 3 classes:\n",
    "1. Model_Servicing Class: this class aim to provide a range of \"services\" for a deep leaning model created from the TensorFlow Framework. Specifically, it wraps a user constructed model as its class attributes, and allows easy model compilation, training, making predictions, and returning evaluation matrics.\n",
    "\n",
    "2. NLP_Model-Servicing Class: subclass of Model_Servicing. It inherents from the Model Servicing class, and is customized spacifically for Natrual Language Classification models. (For example, model evaluation in this class implements confusion matrix to produce a list of commonly used matrices for classfication problems; Model compilation utilized binary crossentropy loss function)\n",
    "\n",
    "3. Dset Class: A class that helps cleaning and managing training and testing dataset.  \n",
    "\n",
    "#### Main() function\n",
    "- In main() function, we constructed 3 deep-learning models for disaster tweets classfications. A simple, single dense layer model; A single dense layer model with pretained text embedding layer; A 1-dimensional CNN model with pretrained text embedding layer. Then, we initilize 3 instances of the NPL_Model_Servicing class to take care of the compilation, training, and evaluation of the 3 models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFd9u1vgWVcI",
    "outputId": "831bd562-dfb9-4950-a13d-c4a16e0d9545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Created! \n",
      "Epoch 1/3\n",
      "215/215 [==============================] - 21s 94ms/step - loss: 0.6631 - accuracy: 0.5849 - val_loss: 0.6227 - val_accuracy: 0.6273\n",
      "Epoch 2/3\n",
      "215/215 [==============================] - 21s 97ms/step - loss: 0.5598 - accuracy: 0.7557 - val_loss: 0.5110 - val_accuracy: 0.8005\n",
      "Epoch 3/3\n",
      "215/215 [==============================] - 22s 100ms/step - loss: 0.4450 - accuracy: 0.8333 - val_loss: 0.4543 - val_accuracy: 0.8346\n",
      "Model 1 trained!\n",
      "Model 1 Completed\n",
      "Epoch 1/3\n",
      "215/215 [==============================] - 6s 16ms/step - loss: 0.5080 - accuracy: 0.7865 - val_loss: 0.4234 - val_accuracy: 0.8097\n",
      "Epoch 2/3\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.4177 - accuracy: 0.8133 - val_loss: 0.4116 - val_accuracy: 0.8123\n",
      "Epoch 3/3\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.4047 - accuracy: 0.8208 - val_loss: 0.4064 - val_accuracy: 0.8202\n",
      "Model 2 trained!\n",
      "Model 2 Completed\n",
      "Epoch 1/3\n",
      "215/215 [==============================] - 36s 163ms/step - loss: 0.6838 - accuracy: 0.5694 - val_loss: 0.6827 - val_accuracy: 0.5735\n",
      "Epoch 2/3\n",
      "215/215 [==============================] - 32s 147ms/step - loss: 0.6843 - accuracy: 0.5700 - val_loss: 0.6827 - val_accuracy: 0.5735\n",
      "Epoch 3/3\n",
      "215/215 [==============================] - 35s 165ms/step - loss: 0.6846 - accuracy: 0.5700 - val_loss: 0.6823 - val_accuracy: 0.5735\n",
      "Model 3 trained!\n",
      "Model 3 Completed\n",
      "Model 1 Results:\n",
      "{'accuracy': 83.46456692913385, 'precision': 0.8361314027820705, 'recall': 0.8346456692913385, 'f1': 0.8327252921312324}\n",
      "Model 2 Results:\n",
      "{'accuracy': 82.02099737532808, 'precision': 0.8196477317588005, 'recall': 0.8202099737532809, 'f1': 0.8195109002952334}\n",
      "Model 3 Results:\n",
      "{'accuracy': 57.3490813648294, 'precision': 0.32889171333898226, 'recall': 0.573490813648294, 'f1': 0.4180408433099324}\n",
      "Ni Hao\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from pyexpat import model\n",
    "from sklearn import datasets\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Model_servicing: # parent class to evaluate and save any deep learning model constructed by tf.keras\n",
    "    def __init__(self,model, epoch) -> None:\n",
    "        self.model=model\n",
    "        self.epoch=epoch\n",
    "    \n",
    "    def train_Model(self,Dset):\n",
    "        model_his = self.model.fit(Dset.train_features,\n",
    "                              Dset.train_labels,\n",
    "                              epochs=self.epoch,\n",
    "                              validation_data=(Dset.val_features, Dset.val_labels))\n",
    "\n",
    "\n",
    "    \n",
    "    def evaluation(self,test_set,test_lable):\n",
    "        return self.model.evaluate(test_set, test_lable)\n",
    "    \n",
    "    def prediction(self,input_set):\n",
    "        return self.model.predict(input_set)\n",
    "    \n",
    "    def SaveModel(self,File_Name):\n",
    "        self.model.save(File_Name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NLP_Model_servicing(Model_servicing): # Chiled Class of Model_servicing: model evaluation for this specific problem: NLP/classfication\n",
    "    \n",
    "    def __init__(self,model) -> None:\n",
    "        self.model=model\n",
    "    \n",
    "\n",
    "    def NLP_prediction_helper(pred,true): # a function to generate evaluation matrices for a classification task\n",
    "        # TP: True Positive %\n",
    "        # TN: True negative %\n",
    "        # FP: False Positive %\n",
    "        # FN False negative %\n",
    "        \n",
    "        \n",
    "        \n",
    "        #model accuracy (TP+TN)/(TP+FP+FN+TN)\n",
    "        model_pred_accuracy = accuracy_score(pred, true) * 100\n",
    "        # model precision (TP/TP+FP)\n",
    "        # model Recall   (TP/TP+FN)\n",
    "        # model f1 score 2*(Recall * Precision) / (Recall + Precision)\n",
    "\n",
    "        model_pred_precision, model_recall, model_f1,= precision_recall_fscore_support(pred, true, average=\"weighted\")\n",
    "        \n",
    "        \n",
    "        pred_results_dic = {\"accuracy\": model_pred_accuracy,\n",
    "                  \"precision\": model_pred_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "        \n",
    "        return pred_results_dic\n",
    "\n",
    "\n",
    "    def NLP_prediction(self,input_feature,true_set):\n",
    "        pred=tf.squeeze(tf.round(Model_servicing.prediction(self,input_set=input_feature)))\n",
    "        \n",
    "        # TP: True Positive %\n",
    "        # TN: True negative %\n",
    "        # FP: False Positive %\n",
    "        # FN False negative %\n",
    "        \n",
    "        \n",
    "        \n",
    "        #model accuracy (TP+TN)/(TP+FP+FN+TN)\n",
    "        model_pred_accuracy = accuracy_score(true_set,pred) * 100\n",
    "        # model precision (TP/TP+FP)\n",
    "        # model Recall   (TP/TP+FN)\n",
    "        # model f1 score 2*(Recall * Precision) / (Recall + Precision)\n",
    "\n",
    "        model_pred_precision, model_recall, model_f1,_=precision_recall_fscore_support(true_set,pred,average=\"weighted\")\n",
    "        \n",
    "        \n",
    "        pred_results_dic = {\"accuracy\": model_pred_accuracy,\n",
    "                  \"precision\": model_pred_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "        \n",
    "        return pred_results_dic\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    def NLP_Compile(self):\n",
    "        self.model.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "class Dset:\n",
    "    def __init__(self,path,seed) -> None:\n",
    "        self.seed=seed\n",
    "        self.dataset=pd.read_csv(path)\n",
    "        self.dataset_shuffled=self.dataset.sample(frac=1, random_state=self.seed)\n",
    "        \n",
    "    def Train_Test_split(self,test_size):   \n",
    "        \n",
    "        self.train_features, self.val_features, self.train_labels, self.val_labels = train_test_split(self.dataset_shuffled[\"text\"].to_numpy(),\n",
    "                                                                            self.dataset_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size=test_size,   # dedicate 10% of samples to validation set\n",
    "                                                                            random_state=self.seed) # random state for reproducibility\n",
    "\n",
    "    \n",
    "\n",
    "def Main():\n",
    "\n",
    "    #------------------------------------------------------------------Load Data---------------------------------------------------------------------------------------------\n",
    "\n",
    "    D1=Dset('train.csv',22)\n",
    "    D1.Train_Test_split(test_size=0.1)\n",
    "\n",
    "    print('Dataset Created! ')\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------Model 1 Single Dense Layer Model-------------------------------------------------------------------------\n",
    "    \n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=50000, # set input shape\n",
    "                             output_dim=128, # set size of embedding vector\n",
    "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
    "                             input_length=15, # how long is each input\n",
    "                             name=\"embedding_1\") \n",
    "    \n",
    "\n",
    "    max_vacab_len=50000\n",
    "    max_out_len=50\n",
    "\n",
    "\n",
    "\n",
    "    text_vectorizer = TextVectorization(max_tokens=max_vacab_len, \n",
    "                                    standardize=\"lower_and_strip_punctuation\", \n",
    "                                    split=\"whitespace\",\n",
    "                                    ngrams=None, \n",
    "                                    output_mode=\"int\", \n",
    "                                    output_sequence_length=max_out_len) \n",
    "    \n",
    "    \n",
    "    text_vectorizer.adapt(D1.train_features)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    inputs = layers.Input(shape=(1,), dtype=\"string\") \n",
    "    x = text_vectorizer(inputs) \n",
    "    x = embedding(x) \n",
    "    x = layers.GlobalAveragePooling1D()(x) \n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x) \n",
    "    model_1 = tf.keras.Model(inputs, outputs, name=\"model_1\")\n",
    "\n",
    "    \n",
    "    \n",
    "    M1=NLP_Model_servicing(model_1)\n",
    "\n",
    "    \n",
    "    \n",
    "    M1.epoch=3\n",
    "\n",
    "    \n",
    "\n",
    "    M1.NLP_Compile()\n",
    "\n",
    "   \n",
    "\n",
    "    M1.train_Model(D1)\n",
    "    \n",
    "    print(\"Model 1 trained!\")\n",
    "\n",
    "\n",
    "    M1_result=M1.NLP_prediction(D1.val_features,D1.val_labels)\n",
    "    \n",
    "    print(\"Model 1 Completed\")\n",
    "\n",
    "#-----------------------------------------------------------------------Model 2 Simple Dense + Pretained Universal Sentence Encoder-------------------------------------------------------\n",
    "    \n",
    "    sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[], # shape of inputs coming to our model \n",
    "                                        dtype=tf.string, # data type of inputs coming to the USE layer\n",
    "                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n",
    "                                        name=\"USE\") \n",
    "    \n",
    "\n",
    "    model_2 = tf.keras.Sequential([\n",
    "       sentence_encoder_layer, # take in sentences and then encode them into an embedding\n",
    "       tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "       tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "                             ], name=\"model_2\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    M2=NLP_Model_servicing(model_2)\n",
    "\n",
    "    \n",
    "    \n",
    "    M2.epoch=3\n",
    "\n",
    "    \n",
    "\n",
    "    M2.NLP_Compile()\n",
    "\n",
    "   \n",
    "\n",
    "    M2.train_Model(D1)\n",
    "    \n",
    "    print(\"Model 2 trained!\")\n",
    "\n",
    "\n",
    "    M2_result=M2.NLP_prediction(D1.val_features,D1.val_labels)\n",
    "    \n",
    "    print(\"Model 2 Completed\")\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------Model 3: 1-D CNN Model + Pretained Universal Sentence Encoder-------------------------------------------------------\n",
    "\n",
    "    model_3 = tf.keras.Sequential([\n",
    "              sentence_encoder_layer, # take in sentences and then encode them into an embedding\n",
    "              embedding,\n",
    "              tf.keras.layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\"),\n",
    "              tf.keras.layers.GlobalMaxPool1D(),\n",
    "              tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "              ], name=\"model_3\")\n",
    "\n",
    "    M3=NLP_Model_servicing(model_3)\n",
    "\n",
    "    \n",
    "    \n",
    "    M3.epoch=3\n",
    "\n",
    "    \n",
    "\n",
    "    M3.NLP_Compile()\n",
    "\n",
    "   \n",
    "\n",
    "    M3.train_Model(D1)\n",
    "    \n",
    "    print(\"Model 3 trained!\")\n",
    "\n",
    "\n",
    "    M3_result=M3.NLP_prediction(D1.val_features,D1.val_labels)\n",
    "    \n",
    "    print(\"Model 3 Completed\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Model 1 Results:')\n",
    "    print(M1_result)\n",
    "\n",
    "    print('Model 2 Results:')\n",
    "    print(M2_result)\n",
    "\n",
    "    print('Model 3 Results:')\n",
    "    print(M3_result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Ni Hao')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OOPII Project-NPL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
